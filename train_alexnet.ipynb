{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_deterministic():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "# Deterministic behavior not yet supported by AdaptiveAvgPool2d\n",
    "#set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloaders_cifar10(batch_size, num_workers=0,\n",
    "                            validation_fraction=None,\n",
    "                            train_transforms=None,\n",
    "                            test_transforms=None):\n",
    "\n",
    "    if train_transforms is None:\n",
    "        train_transforms = transforms.ToTensor()\n",
    "\n",
    "    if test_transforms is None:\n",
    "        test_transforms = transforms.ToTensor()\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='data',\n",
    "                                     train=True,\n",
    "                                     transform=train_transforms,\n",
    "                                     download=True)\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(root='data',\n",
    "                                     train=True,\n",
    "                                     transform=test_transforms)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='data',\n",
    "                                    train=False,\n",
    "                                    transform=test_transforms)\n",
    "\n",
    "    if validation_fraction is not None:\n",
    "        num = int(validation_fraction * 50000)\n",
    "        train_indices = torch.arange(0, 50000 - num)\n",
    "        valid_indices = torch.arange(50000 - num, 50000)\n",
    "\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = torch.utils.data.SubsetRandomSampler(valid_indices)\n",
    "\n",
    "        valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=num_workers,\n",
    "                                  sampler=valid_sampler)\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  sampler=train_sampler)\n",
    "\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=num_workers,\n",
    "                                  drop_last=True,\n",
    "                                  shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers,\n",
    "                             shuffle=False)\n",
    "\n",
    "    if validation_fraction is None:\n",
    "        return train_loader, test_loader\n",
    "    else:\n",
    "        return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ac31d381634d158236408879c54e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Set random seed ###\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "##########################\n",
    "### Dataset\n",
    "##########################\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((70, 70)),\n",
    "                                       transforms.RandomCrop((64, 64)),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((70, 70)),\n",
    "                                      transforms.CenterCrop((64, 64)),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "\n",
    "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=2, \n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    validation_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "\n",
      "Image batch dimensions: torch.Size([256, 3, 64, 64])\n",
      "Image label dimensions: torch.Size([256])\n",
      "tensor([0, 2, 3, 5, 4, 8, 9, 6, 9, 7])\n",
      "\n",
      "Validation Set:\n",
      "Image batch dimensions: torch.Size([256, 3, 64, 64])\n",
      "Image label dimensions: torch.Size([256])\n",
      "tensor([6, 9, 3, 5, 7, 3, 4, 1, 8, 0])\n",
      "\n",
      "Testing Set:\n",
      "Image batch dimensions: torch.Size([256, 3, 64, 64])\n",
      "Image label dimensions: torch.Size([256])\n",
      "tensor([2, 6, 3, 1, 1, 1, 1, 2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "print('Training Set:\\n')\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    print(labels[:10])\n",
    "    break\n",
    "    \n",
    "# Checking the dataset\n",
    "print('\\nValidation Set:')\n",
    "for images, labels in valid_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    print(labels[:10])\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "print('\\nTesting Set:')\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.size())\n",
    "    print('Image label dimensions:', labels.size())\n",
    "    print(labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.alexnet import AlexNet\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = AlexNet(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)  \n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/040 | Batch 0000/0175 | Loss: 2.3033\n",
      "Epoch: 001/040 | Batch 0050/0175 | Loss: 2.0391\n",
      "Epoch: 001/040 | Batch 0100/0175 | Loss: 1.9708\n",
      "Epoch: 001/040 | Batch 0150/0175 | Loss: 1.8354\n",
      "Epoch: 002/040 | Batch 0000/0175 | Loss: 1.7406\n",
      "Epoch: 002/040 | Batch 0050/0175 | Loss: 1.5567\n",
      "Epoch: 002/040 | Batch 0100/0175 | Loss: 1.6000\n",
      "Epoch: 002/040 | Batch 0150/0175 | Loss: 1.6515\n",
      "Epoch: 003/040 | Batch 0000/0175 | Loss: 1.5812\n",
      "Epoch: 003/040 | Batch 0050/0175 | Loss: 1.4481\n",
      "Epoch: 003/040 | Batch 0100/0175 | Loss: 1.4361\n",
      "Epoch: 003/040 | Batch 0150/0175 | Loss: 1.6255\n",
      "Epoch: 004/040 | Batch 0000/0175 | Loss: 1.2936\n",
      "Epoch: 004/040 | Batch 0050/0175 | Loss: 1.3204\n",
      "Epoch: 004/040 | Batch 0100/0175 | Loss: 1.4425\n",
      "Epoch: 004/040 | Batch 0150/0175 | Loss: 1.2456\n",
      "Epoch: 005/040 | Batch 0000/0175 | Loss: 1.2653\n",
      "Epoch: 005/040 | Batch 0050/0175 | Loss: 1.2648\n",
      "Epoch: 005/040 | Batch 0100/0175 | Loss: 1.1596\n",
      "Epoch: 005/040 | Batch 0150/0175 | Loss: 1.3190\n",
      "Epoch: 006/040 | Batch 0000/0175 | Loss: 1.1584\n",
      "Epoch: 006/040 | Batch 0050/0175 | Loss: 1.0975\n",
      "Epoch: 006/040 | Batch 0100/0175 | Loss: 1.0795\n",
      "Epoch: 006/040 | Batch 0150/0175 | Loss: 1.1906\n",
      "Epoch: 007/040 | Batch 0000/0175 | Loss: 1.0993\n",
      "Epoch: 007/040 | Batch 0050/0175 | Loss: 1.1407\n",
      "Epoch: 007/040 | Batch 0100/0175 | Loss: 1.0597\n",
      "Epoch: 007/040 | Batch 0150/0175 | Loss: 0.9936\n",
      "Epoch: 008/040 | Batch 0000/0175 | Loss: 1.0146\n",
      "Epoch: 008/040 | Batch 0050/0175 | Loss: 1.1179\n",
      "Epoch: 008/040 | Batch 0100/0175 | Loss: 1.0119\n",
      "Epoch: 008/040 | Batch 0150/0175 | Loss: 0.9590\n",
      "Epoch: 009/040 | Batch 0000/0175 | Loss: 0.9752\n",
      "Epoch: 009/040 | Batch 0050/0175 | Loss: 1.0628\n",
      "Epoch: 009/040 | Batch 0100/0175 | Loss: 1.0035\n",
      "Epoch: 009/040 | Batch 0150/0175 | Loss: 0.8512\n",
      "Epoch: 010/040 | Batch 0000/0175 | Loss: 0.7685\n",
      "Epoch: 010/040 | Batch 0050/0175 | Loss: 0.9644\n",
      "Epoch: 010/040 | Batch 0100/0175 | Loss: 0.8921\n",
      "Epoch: 010/040 | Batch 0150/0175 | Loss: 0.9339\n",
      "Epoch: 011/040 | Batch 0000/0175 | Loss: 0.8966\n",
      "Epoch: 011/040 | Batch 0050/0175 | Loss: 0.8748\n",
      "Epoch: 011/040 | Batch 0100/0175 | Loss: 0.8503\n",
      "Epoch: 011/040 | Batch 0150/0175 | Loss: 0.8678\n",
      "Epoch: 012/040 | Batch 0000/0175 | Loss: 0.8091\n",
      "Epoch: 012/040 | Batch 0050/0175 | Loss: 0.9611\n",
      "Epoch: 012/040 | Batch 0100/0175 | Loss: 0.7843\n",
      "Epoch: 012/040 | Batch 0150/0175 | Loss: 0.8485\n",
      "Epoch: 013/040 | Batch 0000/0175 | Loss: 0.9257\n",
      "Epoch: 013/040 | Batch 0050/0175 | Loss: 0.8141\n",
      "Epoch: 013/040 | Batch 0100/0175 | Loss: 0.8185\n",
      "Epoch: 013/040 | Batch 0150/0175 | Loss: 0.7124\n",
      "Epoch: 014/040 | Batch 0000/0175 | Loss: 0.7327\n",
      "Epoch: 014/040 | Batch 0050/0175 | Loss: 0.7937\n",
      "Epoch: 014/040 | Batch 0100/0175 | Loss: 0.7074\n",
      "Epoch: 014/040 | Batch 0150/0175 | Loss: 0.7422\n",
      "Epoch: 015/040 | Batch 0000/0175 | Loss: 0.7295\n",
      "Epoch: 015/040 | Batch 0050/0175 | Loss: 0.6875\n",
      "Epoch: 015/040 | Batch 0100/0175 | Loss: 0.7805\n",
      "Epoch: 015/040 | Batch 0150/0175 | Loss: 0.6678\n",
      "Epoch: 016/040 | Batch 0000/0175 | Loss: 0.5723\n",
      "Epoch: 016/040 | Batch 0050/0175 | Loss: 0.5773\n",
      "Epoch: 016/040 | Batch 0100/0175 | Loss: 0.5771\n",
      "Epoch: 016/040 | Batch 0150/0175 | Loss: 0.6533\n",
      "Epoch: 017/040 | Batch 0000/0175 | Loss: 0.6168\n",
      "Epoch: 017/040 | Batch 0050/0175 | Loss: 0.5774\n",
      "Epoch: 017/040 | Batch 0100/0175 | Loss: 0.6521\n",
      "Epoch: 017/040 | Batch 0150/0175 | Loss: 0.5692\n",
      "Epoch: 018/040 | Batch 0000/0175 | Loss: 0.5501\n",
      "Epoch: 018/040 | Batch 0050/0175 | Loss: 0.4720\n",
      "Epoch: 018/040 | Batch 0100/0175 | Loss: 0.5672\n",
      "Epoch: 018/040 | Batch 0150/0175 | Loss: 0.5226\n",
      "Epoch: 019/040 | Batch 0000/0175 | Loss: 0.4788\n",
      "Epoch: 019/040 | Batch 0050/0175 | Loss: 0.5778\n",
      "Epoch: 019/040 | Batch 0100/0175 | Loss: 0.5386\n",
      "Epoch: 019/040 | Batch 0150/0175 | Loss: 0.4901\n",
      "Epoch: 020/040 | Batch 0000/0175 | Loss: 0.5136\n",
      "Epoch: 020/040 | Batch 0050/0175 | Loss: 0.4194\n",
      "Epoch: 020/040 | Batch 0100/0175 | Loss: 0.5293\n",
      "Epoch: 020/040 | Batch 0150/0175 | Loss: 0.5736\n",
      "Epoch: 021/040 | Batch 0000/0175 | Loss: 0.5190\n",
      "Epoch: 021/040 | Batch 0050/0175 | Loss: 0.4741\n",
      "Epoch: 021/040 | Batch 0100/0175 | Loss: 0.4410\n",
      "Epoch: 021/040 | Batch 0150/0175 | Loss: 0.6000\n",
      "Epoch: 022/040 | Batch 0000/0175 | Loss: 0.4298\n",
      "Epoch: 022/040 | Batch 0050/0175 | Loss: 0.5731\n",
      "Epoch: 022/040 | Batch 0100/0175 | Loss: 0.4836\n",
      "Epoch: 022/040 | Batch 0150/0175 | Loss: 0.4791\n",
      "Epoch: 023/040 | Batch 0000/0175 | Loss: 0.3557\n",
      "Epoch: 023/040 | Batch 0050/0175 | Loss: 0.4158\n",
      "Epoch: 023/040 | Batch 0100/0175 | Loss: 0.4500\n",
      "Epoch: 023/040 | Batch 0150/0175 | Loss: 0.3928\n",
      "Epoch: 024/040 | Batch 0000/0175 | Loss: 0.3933\n",
      "Epoch: 024/040 | Batch 0050/0175 | Loss: 0.4083\n",
      "Epoch: 024/040 | Batch 0100/0175 | Loss: 0.3602\n",
      "Epoch: 024/040 | Batch 0150/0175 | Loss: 0.4570\n",
      "Epoch: 025/040 | Batch 0000/0175 | Loss: 0.4535\n",
      "Epoch: 025/040 | Batch 0050/0175 | Loss: 0.4308\n",
      "Epoch: 025/040 | Batch 0100/0175 | Loss: 0.4309\n",
      "Epoch: 025/040 | Batch 0150/0175 | Loss: 0.4316\n",
      "Epoch: 026/040 | Batch 0000/0175 | Loss: 0.3327\n",
      "Epoch: 026/040 | Batch 0050/0175 | Loss: 0.5239\n",
      "Epoch: 026/040 | Batch 0100/0175 | Loss: 0.4025\n",
      "Epoch: 026/040 | Batch 0150/0175 | Loss: 0.3544\n",
      "Epoch: 027/040 | Batch 0000/0175 | Loss: 0.2958\n",
      "Epoch: 027/040 | Batch 0050/0175 | Loss: 0.3380\n",
      "Epoch: 027/040 | Batch 0100/0175 | Loss: 0.3719\n",
      "Epoch: 027/040 | Batch 0150/0175 | Loss: 0.3182\n",
      "Epoch: 028/040 | Batch 0000/0175 | Loss: 0.3265\n",
      "Epoch: 028/040 | Batch 0050/0175 | Loss: 0.3289\n",
      "Epoch: 028/040 | Batch 0100/0175 | Loss: 0.2521\n",
      "Epoch: 028/040 | Batch 0150/0175 | Loss: 0.2947\n",
      "Epoch: 029/040 | Batch 0000/0175 | Loss: 0.2849\n",
      "Epoch: 029/040 | Batch 0050/0175 | Loss: 0.2946\n",
      "Epoch: 029/040 | Batch 0100/0175 | Loss: 0.2724\n",
      "Epoch: 029/040 | Batch 0150/0175 | Loss: 0.2722\n",
      "Epoch: 030/040 | Batch 0000/0175 | Loss: 0.2661\n",
      "Epoch: 030/040 | Batch 0050/0175 | Loss: 0.2746\n",
      "Epoch: 030/040 | Batch 0100/0175 | Loss: 0.2041\n",
      "Epoch: 030/040 | Batch 0150/0175 | Loss: 0.2265\n",
      "Epoch: 031/040 | Batch 0000/0175 | Loss: 0.3072\n",
      "Epoch: 031/040 | Batch 0050/0175 | Loss: 0.2343\n",
      "Epoch: 031/040 | Batch 0100/0175 | Loss: 0.4221\n",
      "Epoch: 031/040 | Batch 0150/0175 | Loss: 0.2829\n",
      "Epoch: 032/040 | Batch 0000/0175 | Loss: 0.2221\n",
      "Epoch: 032/040 | Batch 0050/0175 | Loss: 0.2238\n",
      "Epoch: 032/040 | Batch 0100/0175 | Loss: 0.2446\n",
      "Epoch: 032/040 | Batch 0150/0175 | Loss: 0.2298\n",
      "Epoch: 033/040 | Batch 0000/0175 | Loss: 0.2249\n",
      "Epoch: 033/040 | Batch 0050/0175 | Loss: 0.2625\n",
      "Epoch: 033/040 | Batch 0100/0175 | Loss: 0.1710\n",
      "Epoch: 033/040 | Batch 0150/0175 | Loss: 0.2398\n",
      "Epoch: 034/040 | Batch 0000/0175 | Loss: 0.1897\n",
      "Epoch: 034/040 | Batch 0050/0175 | Loss: 0.2123\n",
      "Epoch: 034/040 | Batch 0100/0175 | Loss: 0.1649\n",
      "Epoch: 034/040 | Batch 0150/0175 | Loss: 0.2510\n",
      "Epoch: 035/040 | Batch 0000/0175 | Loss: 0.2194\n",
      "Epoch: 035/040 | Batch 0050/0175 | Loss: 0.2505\n",
      "Epoch: 035/040 | Batch 0100/0175 | Loss: 0.2320\n",
      "Epoch: 035/040 | Batch 0150/0175 | Loss: 0.1747\n",
      "Epoch: 036/040 | Batch 0000/0175 | Loss: 0.1774\n",
      "Epoch: 036/040 | Batch 0050/0175 | Loss: 0.2086\n",
      "Epoch: 036/040 | Batch 0100/0175 | Loss: 0.1572\n",
      "Epoch: 036/040 | Batch 0150/0175 | Loss: 0.1325\n",
      "Epoch: 037/040 | Batch 0000/0175 | Loss: 0.1728\n",
      "Epoch: 037/040 | Batch 0050/0175 | Loss: 0.1480\n",
      "Epoch: 037/040 | Batch 0100/0175 | Loss: 0.1018\n",
      "Epoch: 037/040 | Batch 0150/0175 | Loss: 0.1924\n",
      "Epoch: 038/040 | Batch 0000/0175 | Loss: 0.1552\n",
      "Epoch: 038/040 | Batch 0050/0175 | Loss: 0.1053\n",
      "Epoch: 038/040 | Batch 0100/0175 | Loss: 0.1857\n",
      "Epoch: 038/040 | Batch 0150/0175 | Loss: 0.2466\n",
      "Epoch: 039/040 | Batch 0000/0175 | Loss: 0.1120\n",
      "Epoch: 039/040 | Batch 0050/0175 | Loss: 0.1382\n",
      "Epoch: 039/040 | Batch 0100/0175 | Loss: 0.1712\n",
      "Epoch: 039/040 | Batch 0150/0175 | Loss: 0.1571\n",
      "Epoch: 040/040 | Batch 0000/0175 | Loss: 0.1707\n",
      "Epoch: 040/040 | Batch 0050/0175 | Loss: 0.1294\n",
      "Epoch: 040/040 | Batch 0100/0175 | Loss: 0.1211\n",
      "Epoch: 040/040 | Batch 0150/0175 | Loss: 0.1143\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        # FORWARD AND BACK PROP\n",
    "        output = model(features)\n",
    "\n",
    "        loss = loss_fn(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        # LOGGING\n",
    "\n",
    "        if not batch_idx % 50:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
    "                    % (epoch+1, NUM_EPOCHS, batch_idx,\n",
    "                        len(train_loader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            if isinstance(logits, torch.distributed.rpc.api.RRef):\n",
    "                logits = logits.local_value()\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.79017639160156\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):  # save memory during inference\n",
    "\n",
    "    train_acc = compute_accuracy(model, train_loader, DEVICE)\n",
    "\n",
    "    print(train_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.16999816894531\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.set_grad_enabled(False):  # save memory during inference\n",
    "\n",
    "    test_acc = compute_accuracy(model, test_loader, DEVICE)\n",
    "\n",
    "    print(test_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './alexnet_pretrained/model_acc_72_wd.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
